{
 "metadata": {
  "name": "",
  "signature": "sha256:4c1f6929b554738479e805a0b78335b24b62c31d3c06c44014a8affd6bd9a8fb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Predicting Which Houses Own EVs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of this section is the following: Given the power consumption, is it possible to predict which houses own EVs?\n",
      "\n",
      "This section takes two main approaches:\n",
      "- In part a, predictions are made using aggregate statistics\n",
      "- In part b, predictions are made using the Markov Model from Part II"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as pl\n",
      "import matplotlib.colors as colors\n",
      "import numpy as np\n",
      "import scipy\n",
      "import scipy.stats as stats\n",
      "import scipy.stats.mstats as mstats\n",
      "import scipy.interpolate as interpolate\n",
      "import sklearn.neighbors as neighbors\n",
      "import sklearn\n",
      "import random\n",
      "\n",
      "import util\n",
      "\n",
      "# DataFrame where index = intervals, columns = house ids\n",
      "usages = pd.read_csv('./data/EV_train.csv', index_col='House ID').transpose().dropna()\n",
      "when_charging = pd.read_csv('./data/EV_train_labels.csv', index_col='House ID').transpose()[0:len(usages.index)]\n",
      "\n",
      "# Only use houses that own EVs\n",
      "usages.index = range(len(usages.index))\n",
      "when_charging.index = range(len(when_charging.index))\n",
      "houses = usages.columns\n",
      "\n",
      "# Series where index = house ids, values = Bool\n",
      "houses_with_ev = houses[when_charging.sum() > 0]\n",
      "houses_without_ev = houses[when_charging.sum() == 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part III a - Using Aggregate Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As noted in Part I, houses with EVs tend to use more power on average, and also tend to have higher variances in power consumption.\n",
      "This section elaborates by looking for similar aggregate statistics, but with a focus on separating houses with EVs from those without EVs as reliably as possible. Much of this section is experimental and guided by intuition."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First split into train/test data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.seed(0)\n",
      "train_houses, validate_houses, test_houses = util.split_data(houses, ratios=[0.6, 0.2, 0.2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, get some statistics for each house.\n",
      "The main statistics here are averages of each derivative, weighted by some polynomial k.\n",
      "The statistics are also split by normalization, to see if it helps or not."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "derivatives = util.derivatives(usages)\n",
      "\n",
      "normed_usages = util.normalize_usages(usages)\n",
      "normed_derivatives = util.derivatives(normed_usages)\n",
      "\n",
      "house_stats = pd.DataFrame(index = houses)\n",
      "house_stats['owns_ev'] = when_charging.sum() > 0\n",
      "house_stats['avg'] = usages.apply(np.average)\n",
      "house_stats['std'] = usages.apply(np.std)\n",
      "for k in range(1, 6):\n",
      "    for i,d in enumerate(derivatives):\n",
      "        key = \"d%i_k%i\" % (i, k)\n",
      "        values = d.apply(abs).apply(lambda x: np.power(x, k)).apply(np.average).apply(lambda x: np.power(x, 1.0/k))\n",
      "        house_stats[key] = values\n",
      "for k in range(1, 6):\n",
      "    for i,d in enumerate(normed_derivatives):\n",
      "        key = \"normed_d%i_k%i\" % (i, k)\n",
      "        values = d.apply(abs).apply(lambda x: np.power(x, k)).apply(np.average).apply(lambda x: np.power(x, 1.0/k))\n",
      "        house_stats[key] = values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-548b5297259c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mderivatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnormed_usages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_usages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnormed_derivatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormed_usages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/scott/Development/gridcure/analysis/util.pyc\u001b[0m in \u001b[0;36mderivatives\u001b[1;34m(usages)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0md0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhouse\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mraw_splines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhouse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0md1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhouse\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mraw_splines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhouse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0md2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhouse\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mraw_splines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhouse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0md3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhouse\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mraw_splines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhouse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[0mderivatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/interpolate/fitpack2.pyc\u001b[0m in \u001b[0;36mderivatives\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mderivatives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;34m\"\"\" Return all derivatives of the spline at the point x.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m         \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfitpack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspalde\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eval_args\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mier\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error code returned by spalde: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for (feature, values) in house_stats.iteritems():\n",
      "    if feature != 'owns_ev':\n",
      "        values = (values - np.mean(values)) / np.std(values)\n",
      "        house_stats[feature] = values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Out of all the features generated above, see which statistic is the most accurate using 1D KNN."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "accs_1D = {}\n",
      "stats = house_stats.columns[1:] # take out the 'owns_ev' column\n",
      "for stat in stats:\n",
      "  \n",
      "    X = house_stats.T[train_houses].T[stat].values\n",
      "    y = house_stats.T[train_houses].T['owns_ev'].values.astype('int')\n",
      "\n",
      "    clf = KNeighborsClassifier(n_neighbors=20)\n",
      "    clf.fit(np.array([X]).T, y)\n",
      "    \n",
      "    X = house_stats.T[validate_houses].T[stat].values\n",
      "    y = house_stats.T[validate_houses].T['owns_ev'].values.astype('int')\n",
      "\n",
      "    pred = clf.predict(np.array([X]).T)\n",
      "\n",
      "    accs_1D[stat] = util.acc_info(pred, y)\n",
      "\n",
      "accs_1D = [(k,v) for k,v in accs_1D.iteritems()]\n",
      "accs_1D.sort(key = lambda (k,v): -v['acc'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for (k,v) in accs_1D[:10]:\n",
      "    print \"key: %s, acc: %.3f (%.3f), pos acc: %.3f (%.3f), neg acc: %.3f (%.3f)\" % (k, v['acc'], v['acc_unc'], v['pos_acc'], v['pos_acc_unc'], v['neg_acc'], v['neg_acc_unc'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = house_stats.T[train_houses].T['d1_k5'].values\n",
      "y = house_stats.T[train_houses].T['owns_ev'].values.astype('int')\n",
      "clf = KNeighborsClassifier(n_neighbors=20)\n",
      "clf.fit(np.array([X]).T, y)\n",
      "X = house_stats.T[test_houses].T['d1_k5'].values\n",
      "y = house_stats.T[test_houses].T['owns_ev'].values.astype('int')\n",
      "pred = clf.predict(np.array([X]).T)\n",
      "acc = util.acc_info(pred, y)\n",
      "print util.acc_str(acc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bins = np.linspace(-3, 5, 41)\n",
      "xs = (bins[1:] + bins[:-1])/2\n",
      "h0 = np.histogram(house_stats[house_stats['owns_ev']]['d1_k5'], bins=bins)\n",
      "h1 = np.histogram(house_stats[~house_stats['owns_ev']]['d1_k5'], bins=bins)\n",
      "pl.plot(xs, h1[0], marker='o', color='blue', label='No EV')\n",
      "pl.plot(xs, h0[0], marker='o', color='red', label='Owns EV')\n",
      "pl.xlabel('Average normalized first derivative, weighted k=5')\n",
      "pl.ylabel('Counts')\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It would appear that high k-values provide the most robust classifiers.\n",
      "It would also appear that first and second derivatives provide the most information.\n",
      "\n",
      "Next, see which combination of statistics is most accurate using 2D KNN."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "accs_2D = {}\n",
      "stats = house_stats.columns[1:] # take out the 'owns_ev' column\n",
      "for s1 in range(0, len(stats)-1):\n",
      "    for s2 in range(s1, len(stats)):\n",
      "        stat1 = stats[s1]\n",
      "        stat2 = stats[s2]\n",
      "\n",
      "        X = zip(house_stats.T[train_houses].T[stats[s1]].values, house_stats.T[train_houses].T[stats[s2]].values)\n",
      "        y = house_stats.T[train_houses].T['owns_ev'].values.astype('int')\n",
      "\n",
      "        clf = KNeighborsClassifier()\n",
      "        clf.fit(X, y)\n",
      "\n",
      "        X = zip(house_stats.T[validate_houses].T[stats[s1]].values, house_stats.T[validate_houses].T[stats[s2]].values)\n",
      "        y = house_stats.T[validate_houses].T['owns_ev'].values.astype('int')\n",
      "\n",
      "        pred = clf.predict(X)\n",
      "\n",
      "        accs_2D[(stat1,stat2)] = util.acc_info(pred, y)\n",
      "\n",
      "accs_2D = [(k,v) for k,v in accs_2D.iteritems()]\n",
      "accs_2D.sort(key = lambda (k,v): -v['acc'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for (k,v) in accs_2D[:10]:\n",
      "    print \"key: %s, acc: %.3f (%.3f), pos acc: %.3f (%.3f), neg acc: %.3f (%.3f)\" % (k, v['acc'], v['acc_unc'], v['pos_acc'], v['pos_acc_unc'], v['neg_acc'], v['neg_acc_unc'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = zip(house_stats.T[train_houses].T['d3_k4'].values, house_stats.T[train_houses].T['d2_k5'].values)\n",
      "y = house_stats.T[train_houses].T['owns_ev'].values.astype('int')\n",
      "clf = KNeighborsClassifier()\n",
      "clf.fit(X,y)\n",
      "X = zip(house_stats.T[test_houses].T['d3_k4'].values, house_stats.T[test_houses].T['d2_k5'].values)\n",
      "y = house_stats.T[test_houses].T['owns_ev'].values.astype('int')\n",
      "pred = clf.predict(X)\n",
      "acc = util.acc_info(pred, y)\n",
      "print util.acc_str(acc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After doing cross-validation, it appears the accuracy of using 2D is only around 81%.\n",
      "It turns out 1D KNN is better than any pair of 2D KNN!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part b - Using the Markov Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Markov Model from Part II was put into a file `markov.py`, which provides an easy interface for training and running."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from markov import EVMarkovModel\n",
      "\n",
      "random.seed(0)\n",
      "# want to only train the Markov Model with houses that own EVs, while also keeping the ratio (with/without ev) correct in the test data\n",
      "train_markov, train_classifier, test_classifier = util.split_data(houses, ratios=[0.4, 0.4, 0.2])\n",
      "run_markov = train_classifier.append(test_classifier)\n",
      "#train_markov, train_classifier, test = util.split_data(houses, ratios=[0.4, 0.4, 0.2])\n",
      "\n",
      "# train the markov model transition probabilities\n",
      "model = EVMarkovModel(usages, when_charging)\n",
      "model.train(train_markov, normed=True)\n",
      "\n",
      "# run the markov model on houses to be classified (this part takes a while)\n",
      "preds = pd.DataFrame()\n",
      "probs = pd.DataFrame()\n",
      "for i,house in enumerate(run_markov):\n",
      "    print \"house %i out of %i\" % (i+1, len(run_markov))\n",
      "    (p0, p1) = model.run(house)\n",
      "    probs[house] = p1\n",
      "    preds[house] = (p1 > 0.5).astype('float')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the probabilities of charging are known at each timestep, it should be possible to perform aggregate statistics to see which houses own EVs.\n",
      "For example, houses with EVs should report more charging than houses without EVs.\n",
      "The plot below looks at a histogram of the average fraction of time the Markov Chain reported charging, separated into houses with or without EVs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avg_probs_with_ev = probs[train_classifier.intersection(houses_with_ev)].apply(np.average)\n",
      "avg_probs_without_ev = probs[train_classifier.intersection(houses_without_ev)].apply(np.average)\n",
      "\n",
      "bins = np.linspace(0, 1, 101)\n",
      "xs = (bins[1:] + bins[:-1])/2\n",
      "ys1 = np.histogram(avg_probs_with_ev, bins=bins)[0]\n",
      "ys2 = np.histogram(avg_probs_without_ev, bins=bins)[0]\n",
      "pl.plot(xs, ys2, linestyle='None', marker='.', color='blue')\n",
      "pl.plot(xs, ys1, linestyle='None', marker='.', color='red')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although houses with EVs do report more charging, it doesn't seem signficant enough for an accurate classifier.\n",
      "Below is an attempt at classification using this statistic with 1D KNN, but it is not as accurate as the aggregate models used in part a."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "X = preds[train_classifier].apply(np.sum).values\n",
      "y = (when_charging[train_classifier].sum() > 0)\n",
      "\n",
      "clf = KNeighborsClassifier()\n",
      "clf.fit(np.array([X]).T, y)\n",
      "\n",
      "X = preds[test_classifier].apply(np.sum).values\n",
      "y = (when_charging[test_classifier].sum() > 0)\n",
      "pred = clf.predict(np.array([X]).T)\n",
      "\n",
      "accs = util.acc_info(pred, y)\n",
      "\n",
      "print \"acc: %.3f (%.3f), pos acc: %.3f (%.3f), neg acc: %.3f (%.3f)\" % (accs['acc'], accs['acc_unc'], accs['pos_acc'], accs['pos_acc_unc'], accs['neg_acc'], accs['neg_acc_unc'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most accurate model so far for predicting which houses own EVs has an accuracy of about 84% with an uncertainty of 2%.\n",
      "This was the 1D KNN with first derivative (weighted k=5).\n",
      "\n",
      "All of the most accurate 1D KNN or 2D KNN pairs seemed to rely on first or second derivatives with high k-values.\n",
      "This implies the tails of the distributions had the most significance.\n",
      "This isn't too surprising, since a house with an EV would have more sudden jumps, hence it would have many timeseries with large first or second derivatives.\n",
      "A different approach which extracts or fits the tails could result in a higher accuracy.\n",
      "\n",
      "Using the Markov Models were not that accurate, but the Markov Model was only used in one way (average time spent charging).\n",
      "It could be possible to use the results from the Markov Models in a different way, for example looking at distributions for how long each charge lasted (turn on to turn off), or the total number of charge groups, or the frequency of the charges.\n",
      "Real charges might have different behavior, which could still be encoded in the results of the Markov Model.\n",
      "\n",
      "Finally, combining the two approaches could result in a more accurate multi-dimensional classifier. For example, it might be worth re-running the first 2D KNN with the Markov Model's output as one of the statistics."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}